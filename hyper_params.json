{
    "patch_size": 4,
    "pos_encoding_learnable": false,
    "token_dim": 8,
    "n_heads": 2,
    "encoder_blocks": 3,
    "mlp_dim": 32,
    "batch_size": 128,
    "lr": 0.005,
    "n_epochs": 3
  }